{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DATA_DIR = \"/data/input/hagrid-sample-30k-384p\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport shutil\nimport torch\nimport torchvision\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SOURCE_DIR = \"/kaggle/input/hagrid-sample-30k-384p/hagrid-sample-30k-384p/hagrid_30k\"\nTARGET_DIR = \"/kaggle/working/hagrid_split\"\n\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\nrandom.seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for split in [\"train\", \"val\", \"test\"]:\n    for cls in os.listdir(SOURCE_DIR):\n        os.makedirs(os.path.join(TARGET_DIR, split, cls), exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for cls in os.listdir(SOURCE_DIR):\n    cls_path = os.path.join(SOURCE_DIR, cls)\n    images = [\n        f for f in os.listdir(cls_path)\n        if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n    ]\n\n    random.shuffle(images)\n\n    total = len(images)\n    train_end = int(total * TRAIN_RATIO)\n    val_end = int(total * (TRAIN_RATIO + VAL_RATIO))\n\n    train_imgs = images[:train_end]\n    val_imgs = images[train_end:val_end]\n    test_imgs = images[val_end:]\n\n    for img in train_imgs:\n        shutil.copy(\n            os.path.join(cls_path, img),\n            os.path.join(TARGET_DIR, \"train\", cls, img)\n        )\n\n    for img in val_imgs:\n        shutil.copy(\n            os.path.join(cls_path, img),\n            os.path.join(TARGET_DIR, \"val\", cls, img)\n        )\n\n    for img in test_imgs:\n        shutil.copy(\n            os.path.join(cls_path, img),\n            os.path.join(TARGET_DIR, \"test\", cls, img)\n        )\n\n    print(f\"{cls}: train={len(train_imgs)}, val={len(val_imgs)}, test={len(test_imgs)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_images(path):\n    return sum(\n        len(files)\n        for _, _, files in os.walk(path)\n        if files\n    )\n\nprint(\"Train images:\", count_images(os.path.join(TARGET_DIR, \"train\")))\nprint(\"Validation images:\", count_images(os.path.join(TARGET_DIR, \"val\")))\nprint(\"Test images:\", count_images(os.path.join(TARGET_DIR, \"test\")))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/working/hagrid_split\"\n\nBATCH_SIZE = 16\nNUM_WORKERS = 2\nIMG_SIZE = 224","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_test_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = datasets.ImageFolder(\n    root=f\"{DATA_DIR}/train\",\n    transform=train_transforms\n)\n\nval_dataset = datasets.ImageFolder(\n    root=f\"{DATA_DIR}/val\",\n    transform=val_test_transforms\n)\n\ntest_dataset = datasets.ImageFolder(\n    root=f\"{DATA_DIR}/test\",\n    transform=val_test_transforms\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_dataset))\nprint(len(val_dataset))\nprint(len(test_dataset))\nprint(len(train_dataset.classes))\nprint(train_dataset.classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Classes:\", train_dataset.classes)\nprint(\"Number of classes:\", len(train_dataset.classes))\n\nimages, labels = next(iter(train_loader))\nprint(\"Batch shape:\", images.shape)\nprint(\"Labels shape:\", labels.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}