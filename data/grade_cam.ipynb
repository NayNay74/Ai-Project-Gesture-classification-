{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        self._register_hooks()\n\n    def _register_hooks(self):\n        def forward_hook(module, inp, out):\n            self.activations = out.detach()\n\n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0].detach()\n\n        self.target_layer.register_forward_hook(forward_hook)\n        self.target_layer.register_backward_hook(backward_hook)\n\n    def __call__(self, x):\n        self.model.zero_grad()\n        output = self.model(x)\n\n        if isinstance(output, tuple):\n            output = output[0]\n\n        class_idx = output.argmax(dim=1).item()\n        score = output[:, class_idx]\n        score.backward()\n\n        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n        cam = (weights * self.activations).sum(dim=1)\n\n        cam = F.relu(cam)\n        cam = cam.squeeze().cpu().numpy()\n        cam -= cam.min()\n        cam /= cam.max() + 1e-8\n\n        return cam\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ninception = models.inception_v3(\n    weights=None,\n    aux_logits=True\n)\n\nnum_classes = len(train_dataset_inception.classes)\n\ninception.fc = nn.Sequential(\n    nn.Linear(inception.fc.in_features, 512),\n    nn.ReLU(),\n    nn.Dropout(0.4),\n    nn.Linear(512, num_classes)\n)\n\n# Load trained weights\ninception.load_state_dict(\n    torch.load(\"inception_best.pth\", map_location=DEVICE)\n)\n\ninception = inception.to(DEVICE)\ninception.eval()\n\n# Grad-CAM setup\ngrad_cam = GradCAM(\n    model=inception,\n    target_layer=inception.Mixed_7c\n)\n\nprint(\"InceptionV3 loaded (Grad-CAM ready)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncomparison_df.set_index(\"Model\")[[\n    \"Accuracy\",\n    \"F1-score (macro)\"\n]].plot(\n    kind=\"bar\",\n    figsize=(8,5),\n    title=\"Comparison of CNN Architectures on Test Set\"\n)\n\nplt.ylabel(\"Score\")\nplt.ylim(0, 1)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ninception = models.inception_v3(\n    weights=None,\n    aux_logits=True\n)\n\nnum_classes = len(train_dataset_inception.classes)\n\ninception.fc = nn.Sequential(\n    nn.Linear(inception.fc.in_features, 512),\n    nn.ReLU(),\n    nn.Dropout(0.4),\n    nn.Linear(512, num_classes)\n)\n\n# Load trained weights\ninception.load_state_dict(\n    torch.load(\"inception_best.pth\", map_location=DEVICE)\n)\n\ninception = inception.to(DEVICE)\ninception.eval()\n\n# Grad-CAM setup\ngrad_cam = GradCAM(\n    model=inception,\n    target_layer=inception.Mixed_7c\n)\n\nprint(\"InceptionV3 loaded (Grad-CAM ready)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport random\n\ninception.eval()\n\nsamples = random.sample(range(len(test_dataset_inception)), 5)\n\nfor idx in samples:\n    image, label = test_dataset_inception[idx]\n\n    input_tensor = image.unsqueeze(0).to(DEVICE)\n\n    cam = grad_cam(input_tensor)\n\n    img = image.permute(1, 2, 0).cpu().numpy()\n    img = (img - img.min()) / (img.max() - img.min())\n\n    cam_resized = cv2.resize(cam, (img.shape[1], img.shape[0]))\n    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n\n    overlay = heatmap * 0.4 + img\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(cam_resized, cmap=\"jet\")\n    plt.title(\"Grad-CAM\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(overlay)\n    plt.title(\"Overlay\")\n    plt.axis(\"off\")\n\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}